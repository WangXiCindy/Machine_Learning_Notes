# 决策树

## 基本流程

- 基于树结构进行决策

  <img src="/assets/MLpics/T43.png" style="zoom:50%;" />

- 树结构

  - 结点：包含的样本集合根据属性测试的结果被划分到子节点中
    - 叶结点：决策结果
    - 其余结点：对应一个属性测试
    - 根结点：包含样本全集
  - 路径
    - 从根结点到每个叶结点的路径对应了一个判定测试序列

- 目的

  - 产生一棵泛化能力强，即处理未见示例能力强的决策树

- 流程：**分而治之（divide-and-cunquer）**

  ​	<img src="/assets/MLpics/T44.png" style="zoom:40%;" />

  - 递归返回的情况
    - 当前结点包含的样本全部属于同一类别，无需划分
    - 当前属性集为空/所有样本在所有样本在所有属性上取值相同，无法划分
      - 把当前结点标记为叶结点
      - 并将其类别设定为该结点所含样本最多的类别
      - 利用当前结点的后验分布
    - 当前结点包含的样本集合为空，不能划分
      - 把当前结点标记为叶结点
      - 将其类别设定为其父结点所含样本最多的类别
      - 把父结点的样本分布作为当前结点的先验分布

## 划分选择

- 决策树的关键是上图（4.2）中的第8行-----如何选择最优划分属性

  - 随着划分的不断进行，希望决策树的分支结点所包含的样本尽可能属于同一类别
  - 结点的“纯度”越来越高

- **信息增益**

  - **信息熵**：度量样本集合纯度最常用的一种指标

  - 信息熵定义为

    - 假定当前样本集合D中第k类样本所占的比例为$p_k（k=1,2,...,\vert y \vert）$

      $Ent(D)=-\sum_{k=1}^{\vert y \vert} p_klog_2p_k$

    - $Ent(D)$的值越小，D的纯度越高

    - 约定若$p=0,plog_2p=0$

    - 最小值为0，最大值为$log_2 \vert y \vert$

  - 信息增益

    - 假定离散属性a有V个可能的取值$a^1,a^2,...,a^V$

    - 若使用a来对样本集D进行划分，则会产生V个分支结点

    - 第V个分支结点包含了D中所有在属性a熵取值为$a^v$的样本，也就是$D^v$

    - 计算出$D^v$的信息熵

    - 由于不同的分支结点所包含的样本数不同，给分支结点赋予权重$\vert D^v \vert / \vert D \vert$

    - 样本数越多的分支结点影响越大

    - 属性a对样本集D进行划分所获得的信息增益

      $Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v)$

    - 信息增益越大，使用属性a进行划分所获得的“纯度提升”越大

      - 信息增益越大，$Ent(D)$越小，$\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v)$越大
      - 总纯度越低，v划分纯度越高

    - 我们可以用信息增益来进行决策树的划分属性选择

      - 图4.2第八行选择$a_*=arg_{a \in A}maxFain(D,a)$
      - 例如ID3决策树学习算法

    - 案例：

      <img src="/assets/MLpics/T45.png" style="zoom:50%;" />

      - 包含17个训练样例，$\vert y \vert =2$

      - $p_1=\frac{8}{17},p_2=\frac{9}{17}$

        $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_k log_2 p_k \\ =&-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})\\ &=0.998 \end{aligned}$$

      - 计算出当前属性集合中每个属性的信息增益

        - {色泽，根蒂，敲声，纹理，脐部，触感}

        - 如：色泽取值：{青绿，乌黑，浅白}

          - D1(色泽=青绿)

            - 样例：{1,4,6,10,13,17}

            - 其中正例3，反例3

            - $p_1=\frac{3}{6},p_2=\frac{3}{6}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})\\ &=1.000 \end{aligned}$$

          - D2(色泽=乌黑)

            - 样例：{2,3,7,8,9,15}

            - 其中正例4，反例2

            - $p_1=\frac{4}{6},p_2=\frac{2}{6}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})\\ &=0.918 \end{aligned}$$

          - D3(色泽=浅白)

            - 样例：{4,11,12,14,16}

            - 其中正例1，反例4

            - $p_1=\frac{1}{5},p_2=\frac{4}{5}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{4}{5}log_2\frac{4}{5}+\frac{1}{5}log_2\frac{1}{5})\\ &=0.722 \end{aligned}$$

        - 当前的信息增益

          $$\begin{aligned}Gain(D,a)&=Ent(D)-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v) \\ &=0.998-(\frac{6}{17} \times 1+\frac{6}{17} \times 0.918+\frac{5}{17} \times 0.722)  \\ &= 0.109\end{aligned}$$

          <img src="/assets/MLpics/T46.png" style="zoom:50%;" />

        - 属性“纹理”的信息增益最大，被选为划分属性

          <img src="/assets/MLpics/T47.png" style="zoom:50%;" />

        - 然后分别计算这三个类别的剩下六个属性的信息增益做进一步划分

          <img src="/assets/MLpics/T48.png" style="zoom:50%;" />

          <img src="/assets/MLpics/T49.png" style="zoom:50%;" />

- **增益率**

  - 如果我们把编号也作为一个候选划分属性，则计算其信息增益为0.998

    - 因为编号产生了17个分支，每个分支仅包含一个样本
    - 这些分支结点的纯度已经达到最大
    - 决策树不具有泛化能力，无法对新样本进行有效预测

  - 信息增益准则对可取值数目较多的属性有所偏好

    - **C4.5**决策树不直接使用信息增益，而是使用增益率来选择最优划分属性

  - 增益率定义为

    $Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$

    - 属性a的**固有值（intrinsic value）**

      $IV(a)=-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}log_2\frac{\vert D^v \vert}{\vert D \vert}$

      - V越大，IV(a)的值越大

  - 增益率准则对可取值数目较少的属性有所偏好

  - C4.5并不是直接选择增益率最大的候选划分属性

    - 使用了**启发式**：从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

- 基尼指数（Gini index）

  - 数据集D的纯度可用基尼值来度量

    $$\begin{aligned}Gini(D) &=\sum^{\vert y \vert}_{k=1}\sum_{k'\not=k}p_kp_{k'} \\ &= 1-\sum^{\vert y \vert}_{k=1}p_k^2\end{aligned}$$

  - Gini(D)反映了从数据集中随意抽取两个样本，其类别标记不一致的概率

  - Gini(D)越小，数据集D的纯度越高

  - $Gini \_index(D,a)=\sum_{v=1}^V\frac{\vert D^v \vert}{\vert D \vert}Gini(D^v)$

  - **CART**决策树

    - 不会严格按照上面公式来选择最优划分属性

    - CART决策树是一颗二叉树

    - 构造算法

      - 对每个属性a的可能取值v，将D分为$a=v,a\not=v$两部分

        $Gini \_index(D,a)=\frac{\vert D^{a=v} \vert}{\vert D \vert}Gini(D^{a=v})+\frac{\vert D^{a\not=v} \vert}{\vert D \vert}Gini(D^{a\not=v})$

      - 选择基尼指数最小的属性作为最优划分属性（其取值为最优划分点）

      - 重复以上两步

    - 案例

      - 色泽取值：{青绿，乌黑，浅白}

        - D1(色泽=青绿)
          - 样例：{1,4,6,10,13,17}
          - 其中正例3，反例3
          - $p_1=\frac{3}{6},p_2=\frac{3}{6}$
        - D2(色泽 !=青绿)
          - 样例：{2,3,5,7,8,9,11,12,14,15,16}
          - 其中正例5，反例6
          - $p_1=\frac{5}{11},p_2=\frac{6}{11}$
        - $Gini \_index(D,a)=\frac{6}{17}(1-(\frac{3}{6})^2-(\frac{3}{6})^2)+\frac{11}{17}(1-(\frac{5}{11})^2-(\frac{6}{11})^2)=0.497$

        <img src="/assets/MLpics/T50.png" style="zoom:50%;" />

  - 在候选集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性

    - $a_*=arg_{a \in A}min Gini\_index(D,a)$

## 剪枝处理

- 决策树学习算法对付“过拟合”的主要手段

- 决策树学习中可能会造成分支过多的问题，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合

- 剪枝策略

  - **预剪枝（prepruning）**

    - 案例：根据信息增益准则

      - 选择“脐部”来进行划分，产生三个分支

        <img src="/assets/MLpics/T51.png" style="zoom:50%;" />

      - 对划分前后的泛化性能进行估计

        - 划分之前，所有样例集中在根结点，若不进行划分，则该结点将被标记为叶结点，其类别标记为训练样例数最多的类别
          - 若我们将这个叶结点标记为“好瓜”
          - 则编号为{4,5,8}的样例被分类正确，另外4个样例分类错误
          - 验证集精度为3/7 x 100%=42.9%

      - 图4.6中的结点2、3、4

        - 分别包含编号为{1,2,3,14}、{6,7,15,17}、{10,16}的训练样例
        - 这三个结点分别被标记为“好瓜”，“好瓜”，“坏瓜”
        - 此时验证集中编号为{4,5,8,11,12}的样例被分类正确
        - 验证集精度为5/7 x 100%=71.4%

      - 结点2进行划分

        - 基于信息增益准则挑选出“色泽”
        - 编号为{5}的验证集样本分类由正确转为错误，精度下降
        - 禁止结点2被划分

      - 结点3进行划分

        - “根蒂”
        - 验证集精度不变，不能提升
        - 禁止结点3被划分

      - 结点4所含训练样例已属于同一类，不再进行划分

      - 结果：仅有一层划分的决策树，别名**“决策树桩”（decision stump）**

    - 优缺点

      - 优点：使得决策树的很多分支都没有“展开”
        - 降低了过拟合的风险
        - 显著减少了训练时间和测试时间开销
      - 缺点：
        - 在当前划分基础上的后续划分却有可能导致性能显著提高
        - 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

  - **后剪枝（post-pruning）**

    - 先从训练集生成一棵完整决策树，如上图4.5

      <img src="/assets/MLpics/T52.png" style="zoom:50%;" />

    - 首先考察结点6

      - 将其领衔的分支剪除，相当于把6替换为叶结点
      - 替换后的叶结点包含编号为{7,15}的训练样本
      - 该类别标记为“好瓜”
      - 决策树的验证集精度提高至57.1%
      - 决定剪枝

    - 结点5

      - 其领衔的子树替换为叶结点
      - 替换后的叶结点包含编号为{6,7,15}的训练样例
      - 叶结点类别标记为“好瓜”
      - 决策树的验证集精度仍为57.1%
      - 不剪枝（但其实根据奥卡姆剃刀准则，应该剪枝，但本书因为绘图不便没有剪枝）

    - 结点2

      - 其领衔的子树替换为叶节点
      - 替换后的叶结点包含编号为{1,2,3,14}的训练样例
      - 叶结点标记为“好瓜”
      - 决策树的验证集精度提高至71.4%
      - 决定剪枝

    - 结点3和1

      - 其领衔的子树替换为叶节点
      - 精度分别为71.4%和42.9%
      - 并未提高，得到保留

    - 结果

      - 验证集精度为71.4%

      <img src="/assets/MLpics/T53.png" style="zoom:50%;" />

    - 优缺点

      - 优点：
        - 欠拟合风险很小
        - 泛化性能优于预剪枝决策树
      - 缺点：
        - 在生成完全决策树之后进行
        - 要自底向上地对树中的所有非叶结点逐一进行考察
        - 训练时间开销比未剪枝决策树和预剪枝决策树都要大得多

## 连续与缺失值

- 连续值处理

  - **二分法**

    - C4.5决策树算法中采用的机制

    - 给定样本集D和连续属性a

    - 假定a在D上出现了n个不同的取值，将它们从小到大进行排序，记为$\{ a^1,a^2,...,a^n \}$

    - 基于划分点t可将D分为子集$D_t^-,D_t^+$

      - $D_t^-$包含那些在属性a上取值不大于t的样本
      - $D_t^+$包含那些在属性a上取值大于t的样本

    - 对于相邻属性取值$a^i,a^{i+1}$来说，t在区间$[a^i,a^{i+1})$中取任意值所产生的划分结果相同

    - 对于连续属性a，我们可考察一个包含n-1个元素的候选划分点集合

      $T_a= \{ \frac{1^i+a^{i+1}}{2} \vert 1 \le i \le n-1\}$

      - 把区间中位点作为候选划分点

      - 像离散属性值一样来考察这些划分点，选取最优

      - 例如针对信息增益公式改造

        $$\begin{aligned} Gain(D,a) &=max_{t \in T_a} Gain(D,a,t) \\ &= max_{t \in T_a} Ent(D)-\sum_{\lambda \in \{ -,+\}}\frac{\vert D^\lambda_t \vert}{\vert D \vert}Ent(D^\lambda_t)\end{aligned}$$

        - 其中Gain(D,a,t)是样本集D基于划分点t二分后的信息增益
        - $\lambda = -$时，$D^\lambda_t=D^{a \le t}_t$
        - $\lambda = +$时，$D^\lambda_t=D^{a > t}_t$

    - 案例：

      - 在西瓜数据集2.0加上连续属性“密度”和“含糖率”，得到西瓜数据3.0

        <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

        - 密度

          - 划分点集合包含16个候选值
          - T密度={0.244,0.294,0.351,0.381,0.42,0.459,0.518,0.574,0.6,0.621,0.636,0.648,0.661,0.681,0.708,0.746}
          - 信息增益为0.262，对应划分点0.381

        - 含糖率

          - 划分点集合包含16个候选值
          - T密度={0.049,0.074,0.095,0.101.0.126,0.155,0.179,0.204,0.213,0.226,0.250,0.265,0.292,0.344,0.373,0.418}
          - 信息增益为0.349，对应划分点0.126

          <img src="/assets/MLpics/T55.png" style="zoom:50%;" />

  - 不同于离散属性，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性

    - 例如在父结点上使用了密度<=0.381，不会禁止在子结点上使用密度<=0.294

- 缺失值处理

  - 如果直接放弃不完整样本

    <img src="/assets/MLpics/T56.png" style="zoom:50%;" />

    - 仅有四个样本可以使用

  - 如何在属性值缺失的情况下进行划分属性选择

    - 给定训练集D和属性a

    - $\tilde{D}$表示D在属性a上没有缺失值的样本子集，仅根据其判断属性a的优劣

    - 假定属性a有V个可取值$\{ a^1,a^2,...,a^n \}$

    - $\tilde{D}^v$表示$\tilde{D}$中在属性a上取值为$a^v$的样本子集

    - $\tilde{D}_k$表示$\tilde{D}$中属于第k类$(k=1,2,....,\vert y \vert)$的样本子集

    - $\tilde{D}=\bigcup_{k=1}^{\vert y \vert} \tilde{D}_{k}$

    - $\tilde{D}=\bigcup_{v=1}^V \tilde{D}^v$

    - 假设我们为每个样本x赋予一个权重$w_x$

    - $\rho=\frac{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in D} w_{\boldsymbol{x}}}$

      - 对属性a，其表示无缺失值样本所占的比例

    - $\tilde{p}_k=\frac{\sum_{\boldsymbol{x} \in \tilde{D}_k} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}$       $(1 \le k \le \vert y \vert)$

      - 对属性a，表示无缺失值样本中第k类所占的吧i里
      - $\sum_{k=1}^{\vert y \vert} \tilde{p}_k=1$

    - $\tilde{r}_v=\frac{\sum_{\boldsymbol{x} \in \tilde{D}^v} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}$          $(1 \le v \le V)$

      - 对属性a，表示无缺失值样本中在属性a上取值$a^v$的样本所占的比例
      - $\sum_{v=1}^V \tilde{r}_v=1$

    - 4.2（信息增益）公式推广

      $$\begin{aligned} Gain(D,a)&=\rho \times Gain(\tilde{D},a) \\ &=\rho \times (Ent(\tilde{D}-\sum_{v=1}^V \tilde{r}_v Ent(\tilde{D}^v)))\end{aligned}$$

    - $Ent(\tilde{D}) = -\sum_{k=1}^{\vert y \vert} \tilde{p}_k log_2 \tilde{p}_k$

  - 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分

    - 若样本x在划分属性a上的取值已知，则将x划入与其取值对应的子结点，且样本权值在子结点中保持为$w_x$
    - 若样本x在划分属性a上的取值未知，则将x同时划入所有子结点，样本权值在与属性值$a^v$对应的子结点中调整为$\tilde{r}_v·w_x$

  - 案例：（以表4.4为例）

    - 开始时，各样例的权值均为1

    - 色泽

      - 该属性上无缺失值的样例子集$\tilde{D}$包含编号为{2,3,4,5,7,8,9,10,11,12,14,15,16,17}

      - $\tilde{D}$的信息熵为

        $$\begin{aligned}Ent(\tilde{D})&=-\sum_{k=1}^2 \tilde{p}_k log_2 \tilde{p}_k \\ &=-(\frac{6}{14}log_2 \frac{6}{14}+ \frac{8}{14}log_2\frac{8}{14}) =0.985\end{aligned}$$

      - 令$\tilde{D}^1,\tilde{D}^2,\tilde{D}^3$分别表示在青绿，乌黑和浅白的样本子集

        - $Ent(\tilde{D}^1)=-(\frac{2}{4}log_2 \frac{2}{4}+\frac{2}{4}log_2 \frac{2}{4})=1.000$
        - $Ent(\tilde{D}^2)=-(\frac{4}{6}log_2 \frac{4}{6}+\frac{2}{6}log_2 \frac{2}{6})=0.918$
        - $Ent(\tilde{D}^3)=-(\frac{0}{4}log_2 \frac{0}{4}+\frac{4}{4}log_2 \frac{4}{4})=0.000$

      - 样本子集$\tilde{D}$上色泽的信息增益为：

        $$\begin{aligned}Gain(\tilde{D},色泽) &= Ent(\tilde{D})-\sum_{v=1}^3 \tilde{r}_v Ent(\tilde{D}^v) \\ &= 0.985-(\frac{4}{14} \times 1.000 + \frac{6}{14} \times 0.918 +\frac{4}{14} \times 0.000) \\ &=0.306\end{aligned}$$

      - 样本集D上色泽的信息增益为：

        $Gain(D,色泽)=\rho \times Gain(\tilde{D},色泽)=\frac{14}{17} \times 0.306 =0.252$

    - <img src="/assets/MLpics/T57.png" style="zoom:50%;" />

    - 纹理信息增益最大，被用于对根结点进行划分

      - 使编号为{1,2,3,4,5,6,15}的样本进入“纹理=清晰”分支

      - {7,9,13,14,17}进入“纹理=稍糊”分支

      - {11,12,16}进入“纹理=模糊”分支

      - 样本在各子结点中的权重保持为1

      - 8样本在“纹理”上出现了缺失值，所以进入三个分支中，权重在三个结点中调整为7/15，5/15，3/15

      - 编号为10的样本与上类似

      - 结果：

        <img src="/assets/MLpics/T58.png" style="zoom:50%;" />

## 多变量决策树

- **轴平行（axis-parallel）**

  - 若我们把每个属性视为坐标空间中的一个坐标轴

  - d个属性描述的样本就对应了d维空间的一个数据点

  - 对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界

  - 轴平行是指它的分类边界由若干个与坐标轴平行的分段组成

  - 如图（只看密度和含糖率）

    <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

  - 生成的决策树和分类边界

    <img src="/assets/MLpics/T59.png" style="zoom:50%;" />

  - 分类边界的每一段都与坐标轴平行，这样的分类边界使学习结构有较好的可解释性

    - 每一段划分都直接对应了某个属性取值

    - 但真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似

    - 如下图，决策树更加复杂，需要更多的属性测试，预测时间开销变大

      <img src="/assets/MLpics/T60.png" style="zoom:50%;" />

- **多变量决策树（multivariate decision tree）**

  - 亦称斜决策树（oblique decision tree）

  - 使用**斜划分**边界，如上图，决策树模型将大为简化

  - 非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试

  - 每个非叶结点是一个形如$\sum_{i=1}^d w_i a_i =t$的线性分类器

    - $w_i和t$可在该结点所含的样本集和属性集上学得

  - 与传统的“单变量决策树（univariate decision tree）”不同，在多变量决策树的学习过程中

    - 不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器

    - 针对西瓜数据3.0

      <img src="/assets/MLpics/T61.png" style="zoom:50%;" />

## 习题

- 4.1 试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为0）的决策树

  - 不含冲突数据
  - 则相同属性特征的样本进入同一叶子结点
  - 必存在与训练集一致的决策树

- 4.2 试析使用“最小训练误差”作为决策树划分选择准则的缺陷

  - 过拟合

- 4.3 试变成实现基于信息熵进行划分选择的决策树算法，并为表4.3中数据生成一棵决策树

  <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

  ```python
  # 根据书中伪代码写function即可
  def treeGenerate(D,A,root,lastNode,lastA):
      flag, category = same_category(D)
      if flag:
          if category==1:
              lastNode[lastA] = '好瓜'  
          else:
              lastNode[lastA] = '坏瓜'
          return
      
      if empty_attribute(A) or sameOnA(D,A):
          lastNode[lastA]=findMost(D)
          return
      
      best_a=findBestA(D,A)
  
      root[best_a]={}
      for av in pd.unique(data.loc[:,best_a]):
          Dv = getDv_a(D,best_a,av)
          if Dv.shape[0]==0:
              root[best_a][av]=findMost(D)
          else:
              A_ = A.drop(best_a)
              root[best_a][av] = {}
              lastA = av
  
              treeGenerate(Dv,A_,root[best_a][av],root[best_a],lastA)
  ```

  ​	结果如下：

  ​	<img src="/assets/Mlpics/T62.png" style="zoom:50%;" />

- 4.4 试实现基于基尼指数进行划分选择的决策树算法，为表4.2中数据生成预剪枝、后剪枝决策树，并与未剪枝决策树进行比较

  - 未剪枝

    ```python
    def treeGenerate(D,A,root,lastNode,lastA):
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        
        best_a=findBestA(D,A)
    
        root[best_a]={}
        for av in pd.unique(data.loc[:,best_a]):
            Dv = getDv_a(D,best_a,av)
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
                
                treeGenerate(Dv,A_,root[best_a][av],root[best_a],lastA)
    ```

  ​	

  - 预剪枝

    ```python
    # 预剪枝
    def prePruning(D,Dtest,A,root,lastNode,lastA):
        # 如果基尼指数为0，即D中样本全属于同一类别，返回
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        best_a=findBestA(D,A)
        
        accCnt=calAccNum(D,Dtest)
        
        # 如果不划分的正确率更高，则不划分
        if cmpAcc(D,Dtest,accCnt,best_a,1):
            root[best_a]={}
        else:
            lastNode[lastA]=findMost(D)
            return
        
        for av in pd.unique(data.loc[:,best_a]):
    
            Dv = getDv_a(D,best_a,av)
            
            Dv_test=getDv_a(Dtest,best_a,av)
            
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
                
                prePruning(Dv,Dv_test,A_,root[best_a][av],root[best_a],lastA)
    ```

    

  - 后剪枝

    ```python
    # 后剪枝
    def postPruning(D,Dtest,A,root,lastNode,lastA):
        
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        
        best_a=findBestA(D,A)
        if lastA!=None:
            lastlastA=lastA
        else:
            lastlastA=None
            
        root[best_a]={}
        for av in pd.unique(data.loc[:,best_a]):
            Dv = getDv_a(D,best_a,av)
            Dv_test=getDv_a(Dtest,best_a,av)
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
    
                postPruning(Dv,Dv_test,A_,root[best_a][av],root[best_a],lastA)
        
    
        # 针对叶子结点开始剪枝
        
        accCnt=calAccNum(D,Dtest)# 计算剪枝的正确率
    
        # 如果不划分的正确率更高，则不划分
        if cmpAcc(D,Dtest,accCnt,best_a,0)==False:
            #lastNode[lastA]=findMost(D)
            lastNode[lastlastA]=findMost(D)
    
            return
    ```

    

  - 结果

    <img src="/assets/MLpics/T63.png" style="zoom:70%;" />

- 4.5 试编程实现基于对率回归进行划分选择的决策树算法，并为表4.3中数据生成一棵决策树

  ```python
  # 对率回归核心算法
  def sigmoid(Z):
      return 1.0/(1+np.exp(-Z))
  
  def gradDescent(data,label,eta=0.1,n_iters=500):
      m,n=data.shape
      label=label.reshape(-1,1)
      
      beta=np.ones((n,1))
      
      for i in range(n_iters):
          y_sig=sigmoid(data.dot(beta))
          m=y_sig-label  #计算误差值
          beta=beta-data.transpose().dot(m)*eta   #误差反传更新参数
          
      return beta
  ```

  ```python
  # 决策树生成算法
  def treeGenerate(D,root,lastNode,lastBeta):
      flag, category = same_category(D)
      if flag:
          if category==1:
              lastNode[lastBeta] = '好瓜'  
          else:
              lastNode[lastBeta] = '坏瓜'
          return
      
      if len(D[0])==1:
          lastNode[lastBeta]=findMost(D)
          return
      
      bestBeta=gradDescent(D[:, :-1], D[:, -1])
  
  
      nodeTxt=""
  
      for i in range(len(bestBeta)):
          if i==0:
              continue
          else:
              nodeTxt+="w"+str(i)+" "+str(bestBeta[i][0])+' \n '
              
      nodeTxt+="<=" + str(-bestBeta[0][0])
      
      root[nodeTxt]={nodeTxt:{}}
      #print(root[nodeTxt],'\n')
      
      Dv_b1,Dv_b2=getDv_b(D,bestBeta)
      class1="是"
      class2="否"
      # 根据beta进行数据集分割
      root[nodeTxt][class1] = {}
      root[nodeTxt][class2] = {}
      treeGenerate(Dv_b1,root[nodeTxt][class1],root[nodeTxt],class1)
      
      treeGenerate(Dv_b2,root[nodeTxt][class2],root[nodeTxt],class2)
  ```

- 4.7 图4.2是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致栈溢出。试使用“队列”数据结构，以参数MaxDepth控制树的最大深度，写出与图4.2等价，但不使用递归的决策树生成算法

  - 如题意，使用队列保存结点
  - 初始化一个队列，并将头结点放入队列中。
  - 用一个while循环，当队列为空时停止。
  - 让一个节点出队列作为当前节点
    - 如果当前节点中的数据都为一类，则把该节点设置为叶子节点。
    - 如果数据集只剩下类，也把当前节点设置为叶子节点，findMaxLabel
  - 如果当前节点的深度小于MaxSize，则继续划分。否则得到叶子节点。

